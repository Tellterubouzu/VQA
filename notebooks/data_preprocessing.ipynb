{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# jsonからcsvに解答データをコンバート"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import re\n",
    "from statistics import mode\n",
    "\n",
    "def process_text(text):\n",
    "    if isinstance(text, list):\n",
    "        text = ' '.join(text)\n",
    "    text = text.lower()\n",
    "    num_word_to_digit = {\n",
    "        'zero': '0', 'one': '1', 'two': '2', 'three': '3', 'four': '4',\n",
    "        'five': '5', 'six': '6', 'seven': '7', 'eight': '8', 'nine': '9',\n",
    "        'ten': '10'\n",
    "    }\n",
    "    for word, digit in num_word_to_digit.items():\n",
    "        text = text.replace(word, digit)\n",
    "    text = re.sub(r'(?<!\\d)\\.(?!\\d)', '', text)\n",
    "    text = re.sub(r'\\b(a|an|the)\\b', '', text)\n",
    "    contractions = {\n",
    "        \"dont\": \"don't\", \"isnt\": \"isn't\", \"arent\": \"aren't\", \"wont\": \"won't\",\n",
    "        \"cant\": \"can't\", \"wouldnt\": \"wouldn't\", \"couldnt\": \"couldn't\"\n",
    "    }\n",
    "    for contraction, correct in contractions.items():\n",
    "        text = text.replace(contraction, correct)\n",
    "    text = re.sub(r\"[^\\w\\s':]\", ' ', text)\n",
    "    text = re.sub(r'\\s+,', ',', text)\n",
    "    text = re.sub(r'\\s+', ' ', text).strip()\n",
    "    return text\n",
    "\n",
    "def clean_text(text):\n",
    "    if isinstance(text, tuple):\n",
    "        text = text[0]\n",
    "    text = re.sub(r\"[\\(\\)\\\"\\',]\", '', text)\n",
    "    return text\n",
    "\n",
    "class VQADatasetProcessor:\n",
    "    def __init__(self, df_path,answer):\n",
    "        self.df = pd.read_json(df_path, convert_axes=False)\n",
    "        self.is_train = answer\n",
    "    \n",
    "    def process_data(self):\n",
    "        processed_data = []\n",
    "        for idx in range(len(self.df)):\n",
    "            question_text = clean_text(process_text(self.df[\"question\"].iloc[idx]))\n",
    "            if self.is_train!=False: \n",
    "                answers = [process_text(answer[\"answer\"]) for answer in self.df[\"answers\"].iloc[idx]]\n",
    "                if len(answers) > 0:\n",
    "                    mode_answer = mode(answers)\n",
    "                    answer_text = clean_text(str(mode_answer))\n",
    "                else:\n",
    "                    answer_text = \"unanswerable\"\n",
    "                processed_data.append({\n",
    "                    'image': self.df['image'].iloc[idx],\n",
    "                    'question': question_text,\n",
    "                    'answer': answer_text\n",
    "                })\n",
    "            else:\n",
    "                processed_data.append({\n",
    "                    'image': self.df['image'].iloc[idx],\n",
    "                    'question': question_text,\n",
    "                })\n",
    "        return processed_data\n",
    "\n",
    "def preprocess_and_save(df_path, output_csv,answer =False):\n",
    "    processor = VQADatasetProcessor(df_path,answer)\n",
    "    processed_data = processor.process_data()\n",
    "    df = pd.DataFrame(processed_data)\n",
    "    df.to_csv(output_csv, index=False)\n",
    "\n",
    "# 前処理を実行してCSVに保存\n",
    "preprocess_and_save('../Data/train.json', '../Data/processed_train.csv',True)\n",
    "preprocess_and_save('../Data/valid.json', '../Data/processed_valid.csv',False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# データの抽出"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "# ファイルの読み込み\n",
    "file_path = '../Data/processed_train.csv'\n",
    "data = pd.read_csv(file_path)\n",
    "\n",
    "# unanswerableの行を75個ランダムに抽出\n",
    "unanswerable_data = data[data['answer'] == 'unanswerable'].sample(n=100, random_state=1)\n",
    "\n",
    "# それ以外の行を125個ランダムに抽出\n",
    "answerable_data = data[data['answer'] != 'unanswerable'].sample(n=300, random_state=1)\n",
    "\n",
    "# データを結合\n",
    "sampled_data = pd.concat([unanswerable_data, answerable_data])\n",
    "\n",
    "# 新しいCSVファイルとして保存\n",
    "output_path = '../Data/extracted_train.csv'\n",
    "sampled_data.to_csv(output_path, index=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 画像データの適応的ヒストグラム平坦化"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "end\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "import numpy as np\n",
    "\n",
    "def apply_clahe_to_color_image(image_path, output_path):\n",
    "    # 画像をカラーで読み込む\n",
    "    image = cv2.imread(image_path)\n",
    "    \n",
    "    # 画像をLAB色空間に変換\n",
    "    lab = cv2.cvtColor(image, cv2.COLOR_BGR2LAB)\n",
    "    \n",
    "    # LAB画像を3つのチャンネルに分割\n",
    "    l, a, b = cv2.split(lab)\n",
    "    \n",
    "    # CLAHEオブジェクトを作成（クリップ制限を設定）\n",
    "    clahe = cv2.createCLAHE(clipLimit=2.0, tileGridSize=(8, 8))\n",
    "    \n",
    "    # LチャンネルにCLAHEを適用\n",
    "    cl = clahe.apply(l)\n",
    "    \n",
    "    # CLAHE適用後のLチャンネルと元のA、Bチャンネルを統合\n",
    "    limg = cv2.merge((cl, a, b))\n",
    "    \n",
    "    # LAB色空間からBGR色空間に変換\n",
    "    final = cv2.cvtColor(limg, cv2.COLOR_LAB2BGR)\n",
    "    \n",
    "    # 画像をシャープ化する\n",
    "    sharpened_image = sharpen_image(final)\n",
    "    \n",
    "    # 画像を保存\n",
    "    cv2.imwrite(output_path, sharpened_image)\n",
    "\n",
    "def sharpen_image(image):\n",
    "    # シャープ化のためのカーネルを定義\n",
    "    # kernel = np.array([[ 0, -0.5,  0],\n",
    "    #                [-0.5,  4, -0.5],\n",
    "    #                [ 0, -0.5,  0]])\n",
    "    kernel = np.array([[ 0, -1,  0],\n",
    "                   [-1,  5, -1],\n",
    "                   [ 0, -1,  0]])\n",
    "\n",
    "    # フィルタを適用して画像をシャープ化\n",
    "    sharpened_image = cv2.filter2D(image, -1, kernel)\n",
    "    return sharpened_image\n",
    "\n",
    "def process_images_in_folder(input_folder, output_folder):\n",
    "    # 入力フォルダ内のすべてのファイルを取得\n",
    "    for filename in os.listdir(input_folder):\n",
    "        if filename.endswith(\".jpg\"):\n",
    "            input_path = os.path.join(input_folder, filename)\n",
    "            output_path = os.path.join(output_folder, filename)\n",
    "            \n",
    "            # CLAHEとシャープ化を適用\n",
    "            apply_clahe_to_color_image(input_path, output_path)\n",
    "            #print(f\"Processed {filename}\")\n",
    "\n",
    "# 入力フォルダと出力フォルダを指定\n",
    "input_folder = \"../Data/valid\"\n",
    "output_folder = \"../Data/shraped_valid\"\n",
    "\n",
    "# 出力フォルダが存在しない場合は作成\n",
    "if not os.path.exists(output_folder):\n",
    "    os.makedirs(output_folder)\n",
    "\n",
    "# フォルダ内の画像を処理\n",
    "process_images_in_folder(input_folder, output_folder)\n",
    "print(\"end\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
